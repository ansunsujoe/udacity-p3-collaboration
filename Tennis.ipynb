{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an Environment Wrapper Class\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "The following classes will create an environment wrapper whose methods will factor in the concept of \"brains\" that exists in the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLEnv:\n",
    "    def __init__(self):\n",
    "        self.n_actions: int = None\n",
    "        self.n_states: int = None\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, action) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "class TennisEnvironment(RLEnv):\n",
    "    def __init__(self, fp:str):\n",
    "        self.unity = UnityEnvironment(file_name=fp)\n",
    "        \n",
    "        # Get the default brain\n",
    "        self.brain_name = self.unity.brain_names[0]\n",
    "        self.brain = self.unity.brains[self.brain_name]\n",
    "        \n",
    "        # Get action space\n",
    "        self.n_actions = self.brain.vector_action_space_size\n",
    "        \n",
    "        # Get state space\n",
    "        env_info = self.unity.reset(train_mode=True)[self.brain_name]\n",
    "        self.n_states = len(env_info.vector_observations[0])\n",
    "        \n",
    "    def reset(self):\n",
    "        env_info = self.unity.reset(train_mode=True)[self.brain_name]\n",
    "        return env_info.vector_observations\n",
    "        \n",
    "    def step(self, action):\n",
    "        env_info = self.unity.step(action)[self.brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        return next_state, reward, done\n",
    "        \n",
    "    def close(self):\n",
    "        self.unity.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the Actor Architecture\n",
    "\n",
    "We will create the actor model architecture. This will be a simple feed-forward neural network that outputs an action given a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Hyperparameters from environment variables\n",
    "        fc1_units = int(os.getenv(\"ACTOR_FC1_UNITS\"))\n",
    "        fc2_units = int(os.getenv(\"ACTOR_FC2_UNITS\"))\n",
    "        fc3_units = int(os.getenv(\"ACTOR_FC3_UNITS\"))\n",
    "        fc4_units = int(os.getenv(\"ACTOR_FC4_UNITS\"))\n",
    "        \n",
    "        # Network components\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, fc4_units)\n",
    "        self.fc5 = nn.Linear(fc4_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return F.tanh(self.fc5(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Critic Architecture\n",
    "\n",
    "We will create the critic model architecture. This will be a simple feed-forward neural network that outputs expected rewards given a state and an action (like a Q-network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Hyperparameters from environment variables\n",
    "        fc1_units = int(os.getenv(\"CRITIC_FC1_UNITS\"))\n",
    "        fc2_units = int(os.getenv(\"CRITIC_FC2_UNITS\"))\n",
    "        fc3_units = int(os.getenv(\"CRITIC_FC3_UNITS\"))\n",
    "        fc4_units = int(os.getenv(\"CRITIC_FC4_UNITS\"))\n",
    "        \n",
    "        # Network components\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, fc4_units)\n",
    "        self.fc5 = nn.Linear(fc4_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the neural network, we will first make sure to establish a connection to the GPU device if we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the Replay Buffer\n",
    "\n",
    "We will next create the replay buffer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        for i in range(len(state)):\n",
    "            e = self.experience(state[i], action[i], reward[i], next_state[i], done[i])\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Class for OU Noise Generation\n",
    "\n",
    "Ornstein-Uhlenbeck process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create the DDPG Agent Class\n",
    "\n",
    "We will be setting up an agent that hosts an actor-critic model (DDPG) and learns from input experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env:RLEnv, seed, lr_actor:float, lr_critic:float, buffer_size:int,\n",
    "                 batch_size:int, gamma:float, tau:float, weight_decay:float=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            env (RLEnv): the environment\n",
    "            seed (int): random seed\n",
    "            lr_actor (float): learning rate of the actor model\n",
    "            lr_critic (float): learning rate of the critic model\n",
    "            buffer_size (int): replay buffer size\n",
    "            batch_size (int): minibatch size for model training\n",
    "            gamma (float): reward discount factor\n",
    "            tau (float): constant for soft update of target parameters\n",
    "            weight_decay (float): L2 weight decay factor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Actor network (local and target)\n",
    "        self.actor_local = Actor(self.env.n_states, self.env.n_actions, seed).to(device)\n",
    "        self.actor_target = Actor(self.env.n_states, self.env.n_actions, seed).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=self.lr_actor)\n",
    "\n",
    "        # Critic network (local and target)\n",
    "        self.critic_local = Critic(self.env.n_states, self.env.n_actions, seed).to(device)\n",
    "        self.critic_target = Critic(self.env.n_states, self.env.n_actions, seed).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=self.lr_critic)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size=self.env.n_actions, \n",
    "                                   buffer_size=self.buffer_size, \n",
    "                                   batch_size=self.batch_size, \n",
    "                                   seed=seed)\n",
    "        \n",
    "        # OU Noise creation\n",
    "        self.noise = OUNoise(size=self.env.n_actions, seed=seed, mu=0, \n",
    "                             theta=float(os.getenv(\"NOISE_THETA\", \"0.15\")),\n",
    "                             sigma=float(os.getenv(\"NOISE_SIGMA\", \"0.2\")))\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Step 1: Update CRITIC\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize critic loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Step 2: Update ACTOR\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize actor loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update for target networks for both actor and critic\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "            \n",
    "    def train_ddpg(self, n_episodes:int, max_t:int=300, solve_score:float=0, plot=True) -> None:\n",
    "        \"\"\"DDPG Learning.\n",
    "    \n",
    "        Params\n",
    "        ======\n",
    "            n_episodes (int): maximum number of training episodes\n",
    "            max_t (int): maximum number of timesteps per episode\n",
    "            solve_score (float): the minimum average score over 100 episodes that is required\n",
    "                in order for the problem to be considered solved\n",
    "        \"\"\"\n",
    "        scores_deque = deque(maxlen=100)\n",
    "        scores = []\n",
    "        \n",
    "        # Iterate through epochs\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            state = self.env.reset()\n",
    "            self.reset()\n",
    "            score = 0\n",
    "            \n",
    "            # Iterate through timesteps until we reach a \"done\" state\n",
    "            for t in range(max_t):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += np.max(reward)\n",
    "                \n",
    "                # Marks the end of an episode\n",
    "                if np.any(done):\n",
    "                    break\n",
    "                \n",
    "            # Append to score window\n",
    "            scores_deque.append(score)\n",
    "            scores.append(score)\n",
    "            \n",
    "            # Print episode statistics\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "            # Did we solve the environment?\n",
    "            if np.mean(scores_deque) >= solve_score:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "                self.save_actor_model(\"checkpoint_actor.pth\")\n",
    "                self.save_critic_model(\"checkpoint_critic.pth\")\n",
    "                break\n",
    "        \n",
    "        # Plot scores if needed\n",
    "        if plot:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(len(scores)), scores)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "            \n",
    "    def save_actor_model(self, fp:str) -> None:\n",
    "        torch.save(self.actor_local.state_dict(), fp)\n",
    "        \n",
    "    def save_critic_model(self, fp:str) -> None:\n",
    "        torch.save(self.critic_local.state_dict(), fp)\n",
    "    \n",
    "    def load_actor_model(self, fp:str) -> None:\n",
    "        self.actor_local.load_state_dict(torch.load(fp))\n",
    "        \n",
    "    def load_critic_model(self, fp:str) -> None:\n",
    "        self.critic_local.load_state_dict(torch.load(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train DDPG on the Continuous Environment\n",
    "\n",
    "We will now initialize a DDPG agent and run it on the Tennis environment. Results will be plotted after the \"train_ddpg\" method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = TennisEnvironment(fp=\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "\n",
    "# Actor/critic model hyperparameters\n",
    "os.environ[\"ACTOR_FC1_UNITS\"] = \"128\"\n",
    "os.environ[\"ACTOR_FC2_UNITS\"] = \"64\"\n",
    "os.environ[\"ACTOR_FC3_UNITS\"] = \"32\"\n",
    "os.environ[\"ACTOR_FC4_UNITS\"] = \"16\"\n",
    "os.environ[\"CRITIC_FC1_UNITS\"] = \"128\"\n",
    "os.environ[\"CRITIC_FC2_UNITS\"] = \"64\"\n",
    "os.environ[\"CRITIC_FC3_UNITS\"] = \"32\"\n",
    "os.environ[\"CRITIC_FC4_UNITS\"] = \"16\"\n",
    "\n",
    "# OU Noise hyperparameters\n",
    "os.environ[\"NOISE_THETA\"] = \"0.15\"\n",
    "os.environ[\"NOISE_SIGMA\"] = \"0.05\"\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(env=env,\n",
    "              seed=0,\n",
    "              lr_actor=7e-4,\n",
    "              lr_critic=8e-4,\n",
    "              buffer_size=100000,\n",
    "              batch_size=32,\n",
    "              gamma=0.95,\n",
    "              tau=1e-3,\n",
    "              weight_decay=0)\n",
    "\n",
    "# Train agent\n",
    "agent.train_ddpg(n_episodes=10000,\n",
    "                max_t=1000,\n",
    "                solve_score=30)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
